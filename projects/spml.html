<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SPML: Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image. 

We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose.">
<meta name="keywords" content="semantic segmentation; metric learning; weakly-supervised learning">
<link rel="author" href="https://twke18.github.io/">

<!-- Fonts and stuff -->
<link href="./spml/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./spml/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./spml/iconize.css">
<script async="" src="./spml/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
  <h1>Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning</h1>

  <div class="authors">
    <a href="https://twke18.github.io/">Tsung-Wei Ke</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://jyhjinghwang.github.io/">Jyh-Jing Hwang</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="http://www1.icsi.berkeley.edu/~stellayu/">Stella X. Yu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </div>

  <div class="affiliations">
    <a href="https://www.berkeley.edu/">UC Berkeley</a> / 
    <a href="https://www.icsi.berkeley.edu/icsi/">ICSI</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </div>

  <div class="venue">International Conference on Learning Representations (<a href="http://iclr.cc/" target="_blank">ICLR</a>) 2021</div>
      </div>


      <center><img src="./spml/main.png" border="0" width="90%"></center>
      <br>
      <center><img src="./spml/teaser.jpg" border="0" width="90%"></center>

<div class="section abstract">
  <h2>Abstract</h2>
  <br>
  <p>
Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image. 
<br>
<br>
We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose.
  </p>
      </div>

<div class="section materials">
  <h2>Materials</h2>
  <center>
    <ul>
      <li class="grid">
        <div class="griditem">
	  <a href="" target="_blank" class="imageLink"><img src="./spml/spml_paper.jpg" border="0"></a><br>
          <a href="" target="_blank">Paper</a>
        </div>
      </li>
      <li class="grid">
        <div class="griditem">
    <a href="./spml/spml_poster.pdf" target="_blank" class="imageLink"><img src="./spml/spml_poster.jpg"></a><br>
          <a href="./spml/spml_poster.pdf" target="_blank">Poster</a>
        </div>
      </li>
    </ul>
  </center>
</div>

<br>

<div class="section code">
  <h2>Code and Models</h2>
  <center>
    <ul>
      <li class="grid">
        <div class="griditem">
	  <a href="https://github.com/twke18/SPML" target="_blank" class="imageLink"><img src="./common/code.png"></a><br>
          <a href="https://github.com/twke18/SPML" target="_blank">Code</a>
        </div>
      </li>
      <li class="grid">
	<div class="griditem">
          <a href="https://drive.google.com/drive/folders/1dUzW1zygAfgDNpK6Yqwa_BdVLLHq5-lo?usp=sharing" target="_blank" class="imageLink"><img src="../img/cal.png"></a><br>
          <a href="https://drive.google.com/drive/folders/1dUzW1zygAfgDNpK6Yqwa_BdVLLHq5-lo?usp=sharing" target="_blank">Models & Masks</a>
        </div>
      </li>
    </ul>
  </center>
</div>

<br>

<div class="section citation">
  <h2>Citation</h2>
  <div class="section bibtex">
    <pre>@inproceedings{ke2021spml,
  title={Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning},
  author={Ke, Tsung-Wei and Hwang, Jyh-Jing and Yu, Stella X},
  booktitle={International Conference on Learning Representations},
  pages={},
  year={2021}
}</pre>
    </div>
</div>

    
<br>

<div class="section materials">
  <h2>Method Highlight</h2>
    <center><img src="./spml/relationships.jpg" border="0" width="90%"></center>
  <br>
  <p>
  <b>Four types of pixel-to-segment attraction and repulsion relationships.</b> A pixel is <span style="color:blue">attracted to</span> (<span style="color:red">repelled by</span>) segments: <b>a)</b> of similar (different) visual appearances such as color or texture, <b>b)</b> of the same (different) class labels, <b>c)</b> in images with common (distinctive) labels, <b>d)</b> of nearby(far-away) feature embeddings. They form different positive and negative sets.
  </p>
</div>

<br>

<div class="section materials">
  <h2>Semantic Segmentation w.r.t Different Types of Annotations</h2>
    <center><img src="./spml/vis_results.jpg" border="0" width="90%"></center>
</div>

<br>
	    
<div class="section acknowledgement">
  <h2>Acknowledgements</h2>
  <br>
  <p>
  This work was supported, in part, by Berkeley Deep Drive and Berkeley AIResearch Commons with Facebook.   This work used the Extreme Science and Engineering Dis-covery Environment (XSEDE), which is supported by National Science Foundation grant numberACI-1548562.  Specifically, it used the Bridges system, which is supported by NSF award numberACI-1445606, at the Pittsburgh Supercomputing Center (PSC).
  </p>
</div>
	

</body></html>
